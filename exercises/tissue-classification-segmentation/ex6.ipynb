{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c09354f",
   "metadata": {},
   "source": [
    "| [![back](../../media/navigation/back.png)](../../exercises/tissue-classification-segmentation/ex5.html) | [![home](../../media/navigation/home.png)](../../index.html) |\n",
    "| :---: | :---: |\n",
    "| Ex 4.5: Segmentation quality control | â€¢ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61829b7e",
   "metadata": {},
   "source": [
    "# 4. Tissue segmentation and classification\n",
    "\n",
    "## 4.6 Area of signal per slide\n",
    "\n",
    "Measuring the area taken by the signal of N channels in a fluo image is a quite common task but several pitfalls must be avoided, the main one being to adjust the brightness and contrast for each image.\n",
    "\n",
    "### 4.6.1 Area of P1, P2 and P3 per slide\n",
    "\n",
    "#### Goals:\n",
    "\n",
    "We want to measure the area covered by the signal on each channel, on the whole images, following several rules. The rules are the following one:\n",
    "- The channel 1 and 2 are mutually exclusive (== there cannot be 1 where there is 2 and vice versa)\n",
    "- The channel 3 is mutually exclusive with 1 and 2 (== if there is 1 or 2, you can't have 3)\n",
    "To satisfy these constraints, we will train a pixel classifier (like for [Ex 4.3: Using a N-classes pixel classifier](../../exercises/tissue-classification-segmentation/ex3.html)) on training image containing representative regions.\n",
    "\n",
    "We will try to find 4 classes: `P1`, `P2`, `P3` and `Ignore*`.\n",
    "\n",
    "#### Required data:\n",
    "\n",
    "| **Folder** | Description | Location | License |\n",
    "| --- | --- | --- | --- |\n",
    "| Area of fluo | Fluo images with three channels containing objects that we will name P1, P2 and P3 | Courtesy of Claire CRAMPES and Tommy CHASTEL, Andrei TURTOI team, IRCM | - |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ca865f",
   "metadata": {},
   "source": [
    "### A. Create a training image\n",
    "\n",
    "- On these images, the whole field contains something, so we can skip the step where we segment the whole organ. We can pass to the training right away. The first step is to create a training image containing representative areas on the slide.\n",
    "- To help you pick your regions, you should visualize your images using the same LUT and brightness and contrast.\n",
    "- To do that, we will start by using the script that you can find in [scripts/utils/ch_name_and_luts.groovy](../../scripts/utils/ch_name_and_luts.groovy). If you load it in QuPath, you can edit the content of `channels_name` to set the channel names to `P1`, `P2` and `P3`. You must also change the content of `LUTs` so the names match the channel names that we just provided. If you whish, you can change the LUTs by providing some RGB triplets as you can find using [any color picker](https://htmlcolorcodes.com/color-picker/).\n",
    "- Once you chose your names and your LUT, run the script **for the whole project**.\n",
    "- You can now open the ![QP channels tools](../../media/qp-icons/channels-tol.png) channels tools, but before adjusting anything, make sure that the \"Apply to similar images\" checkbox (at the very bottom of the floating window) is activated. That will apply the same B&C to all the images of your project.\n",
    "- Using some ![QP rect ann](../../media/qp-icons/rectangle-selection.png) rectangle annotations and the `Region*` class, you can now constitute a representative set of regions.\n",
    "- Once you estimate that your set contains approximately every situation, you can go to the top bar menu and create your training image using \"Classify\" > \"Training images\" > \"Create training image...\". If you used the `Region*` class for your rectangles, you don't have anything to change in the settings.\n",
    "\n",
    "![train img fluo](../../media/tissue-classification/training-fluo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dc354c",
   "metadata": {},
   "source": [
    "### B. Train the classifier\n",
    "\n",
    "- We named our channels `P1`, `P2` and `P3` but it is very unlikely that you have the classes `P1`, `P2` and `P3` in your class list.\n",
    "- To address this situation, you can go to the top of your class list, next to the \"+\" and \"-\" button you should have a little triangular button. If you click on it, you should see the option \"Populate from image channels\". If you launch it, the list should now contain the classes that we need.\n",
    "- Activate the ![QP point cloud](../../media/qp-icons/points-selection.png) point clouds tool and using the \"Add\" button, create 4 point clouds. Give the classes `P1`, `P2` and `P3` to three of them, and `Ignore*` to the last one (that will contain the background).\n",
    "- Like we did at the [Ex 4.3: Using a N-classes pixel classifier](../../exercises/tissue-classification-segmentation/ex3.html), start by providing betwen 10 and 20 points per class using the rules mentionned in the \"Goals\" section of this exrcise.\n",
    "- Then, go to \"Classify\" > \"Pixel classification\" > \"Train pixel classifier...\"\n",
    "- Start by choosing the algorithm (Random trees) and adjusting the resolution. If you look at the images, you will notice that the signal of P2 makes small spots of a few pixels of radius in certain situations, so we need to work at full resolution.\n",
    "- With the \"Live prediction\" ativated, you should now choose your features and their radii by checking whether they are useful or not.\n",
    "- Iteratively, you will add points until the result looks satisfying. Don't forget to keep the number of points in each class approximately equal.\n",
    "- Finish by naming and saving your classifier (and your project). For this example, we named the classifier \"find-dyes\".\n",
    "\n",
    "![preview fluo](../../media/tissue-classification/preview-fluo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca43fc0a",
   "metadata": {},
   "source": [
    "### C. Use the classifier to measure areas\n",
    "\n",
    "#### a. On one image\n",
    "\n",
    "- Go on any image of the project.\n",
    "- If some `Region*` annotation is present, remove it using the same technique we did before:\n",
    "    - ![Kenney-rc](../../media/inputs-icons/mouse_right.svg) right-click on the `Region*` class in the list.\n",
    "    - \"Select objects by classification\"\n",
    "    - Click on \"Delete\"\n",
    "- The whole field is filled with some objects but QuPath is only able to launch classifiers in annotations for measurements, what can we do then?\n",
    "- In the top bar menu, go to \"Objects\" > \"Annotations\" > \"Create full image annotation\".\n",
    "- With the annotation still active, go to \"Classify\" > \"Pixel classification\" > \"Load pixel classifier...\"\n",
    "- In the dropdown menu, choose the classifier that we just trained.\n",
    "    - Using the \"Create objects\" button, create **detections** for each dye (don't split them, don't remove debris, don't fill holes).\n",
    "    - Using the \"Add measurement\" button, inject the area of each dye in the parent annotation's data.\n",
    "- You can close the \"Load pixel classifier\" window and save your project.\n",
    "\n",
    "#### b. On the project\n",
    "\n",
    "- Go to the \"Workflow\" tab and turn your commands history into a script using \"Create script\".\n",
    "- Among all the lines that you have, you should only keep the five ones to:\n",
    "    - Select and remove the `Region*` annotations.\n",
    "    - Create the full image annotation and make it active (it's what the 'true' is for in the parenthesis).\n",
    "    - Create detections using the pixels classifier inside the current annotation.\n",
    "    - Create measurements using the pixels classifier inside the current annotation.\n",
    "- Your final script should look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5914a5ae",
   "metadata": {},
   "source": [
    "```groovy\n",
    "selectObjectsByClassification(\"Region*\");\n",
    "removeSelectedObjects()\n",
    "createFullImageAnnotation(true)\n",
    "createDetectionsFromPixelClassifier(\"find-dyes\", 0.0, 0.0)\n",
    "addPixelClassifierMeasurements(\"find-dyes\", \"find-dyes\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6a93f5",
   "metadata": {},
   "source": [
    "- Don't forget to save your project as well as the script at this point.\n",
    "- Using the ![QP more opt](../../media/qp-icons/more-options.png) \"more options\" buttons, run the script for all the images of this project (except the sparse image and the one you processed manually).\n",
    "- Save your project once the processing is over."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fb183a",
   "metadata": {},
   "source": [
    "### D. Export your measurements\n",
    "\n",
    "- If you saved your project, you can go to \"Measure\" > \"Export measurements...\".\n",
    "- Move to the right column all the images on which you ran the analysis.\n",
    "- Provide a path for your TSV file.\n",
    "- The informations we are interested in (the area occupied by each dye) is a property of annotations. You can verify that by clicking on any full-image annotation and looking in its properties in the lower left panel of QuPath.\n",
    "- If you export your measurements, open them in a spread sheet and remove the useless columns, you should get something similar to: \n",
    "\n",
    "![final results fluo](../../media/tissue-classification/result-fluo.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
